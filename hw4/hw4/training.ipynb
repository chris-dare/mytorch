{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "   \n",
    "from tests import test_prediction, test_generation\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# readable_vocab = pd.read_csv('../dataset/vocab.npy', encoding = \"utf-8\")\n",
    "# readable_vocab.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = [torch.from_numpy(data) for data in dataset]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size:int, seq_len:int, shuffle:bool=True):\n",
    "#         super(LanguageModelDataLoader, self).__init__(dataset)\n",
    "        data = torch.cat([torch.from_numpy(record) for record in dataset])\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start = 0\n",
    "        self.end = self.start + self.batch_size\n",
    "#         indices = list(range(len(dataset)))\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "            \n",
    "        # flatten the dataset and reshape it by the sequence length\n",
    "        labels = torch.cat([data[1:], data[0:1]])\n",
    "        valid_tensors = (len(data) - len(data) % seq_len )\n",
    "        shape = (int(valid_tensors / seq_len), seq_len)\n",
    "        reshaped_input = torch.reshape(data[:valid_tensors], shape)\n",
    "        reshaped_labels = torch.reshape(labels[:valid_tensors], shape)\n",
    "        new_indices = len(reshaped_labels)\n",
    "        new_indices = list(range(new_indices))\n",
    "        random.shuffle(new_indices)\n",
    "        self.data = [reshaped_input[i] for i in new_indices]\n",
    "        print(len(self.data[0]))\n",
    "        self.labels = [reshaped_labels[i] for i in new_indices]\n",
    "        \n",
    "        # build batches\n",
    "        \n",
    "        num_batches = len(self.data) / self.batch_size\n",
    "        batch_indices = np.arange(start=0, stop=len(self.data), step=3)\n",
    "        \n",
    "       \n",
    "        \n",
    "    def test(self):\n",
    "        return self.data, self.labels\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        start = 0\n",
    "        end = start + self.batch_size\n",
    "        iterations = len(self.dataset)/self.batch_size\n",
    "        iterations = int(iterations)\n",
    "        while i <= iterations:\n",
    "            batch = self.dataset[start:end]\n",
    "            if len(batch)< self.batch_size:\n",
    "                batch = self.dataset[-self.batch_size:]\n",
    "            yield torch.cat(batch)\n",
    "            start = end\n",
    "            \n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        num_batches = len(self.data) / self.batch_size\n",
    "        num_batches = int(num_batches)\n",
    "        i = -1\n",
    "        while i < num_batches - 1:\n",
    "            i += self.batch_size\n",
    "            yield self.data[i]\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "self = LanguageModelDataLoader(dataset=dataset[:30],seq_len=5, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     5,    10, ..., 14310, 14315, 14320])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = len(self.data) / self.batch_size\n",
    "batch_indices = np.arange(start=0, stop=len(self.data), step=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1420, 13859,  3714, ...,   813,    79,  1417], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3308)\n"
     ]
    }
   ],
   "source": [
    "class Foo:\n",
    "    def __init__(self):\n",
    "        self.data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    def __iter__(self):\n",
    "        print(\"Whatever we do here executes once\")\n",
    "        i = -1\n",
    "        while i < len(self.data) - 1:\n",
    "            i += 1\n",
    "            yield self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiTextDataset = WikiTextDataset(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(11)[1:]\n",
    "labels = torch.cat([a[1:], a[0:1]])\n",
    "seq_len =3\n",
    "valid_tensors = (len(a) - len(a) % seq_len )\n",
    "shape = (seq_len, int(valid_tensors / seq_len))\n",
    "reshaped_input = torch.reshape(a[:valid_tensors], shape)\n",
    "reshaped_labels = torch.reshape(labels[:valid_tensors], shape)\n",
    "new_indices = len(reshaped_labels)\n",
    "new_indices = list(range(new_indices))\n",
    "random.shuffle(new_indices)\n",
    "data = [reshaped_input[i] for i in new_indices]\n",
    "labels = [reshaped_labels[i] for i in new_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [1,2,4]\n",
    "random.shuffle(temp)\n",
    "temp\n",
    "shuffled_indices\n",
    "data = [reshaped_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(11)[1:]\n",
    "p2d = (0,0, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
    "b = nn.functional.pad(a, p2d, 'constant', 0)\n",
    "b.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([torch.from_numpy(data) for data in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text_dataset = WikiTextDataset(dataset)\n",
    "wiki_text_dataloader = DataLoader(dataset=wiki_text_dataset, shuffle=True, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(wiki_text_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        raise NotImplemented\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        raise NotImplemented\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiUrjbEjSwRQ"
   },
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = None\n",
    "BATCH_SIZE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D8wTJkBSwRc"
   },
   "outputs": [],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
